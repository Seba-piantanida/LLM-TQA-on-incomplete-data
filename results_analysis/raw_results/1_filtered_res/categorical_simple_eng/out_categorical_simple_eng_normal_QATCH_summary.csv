table_name,model,avg_valid_efficiency_score,avg_cell_precision,avg_cell_recall,avg_execution_accuracy,avg_tuple_cardinality,avg_tuple_constraint,avg_tuple_order
animals,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.7751842105263158,0.8853947368421053,0.42105263157894735,0.994736842105263,0.5447368421052632,0.8434210526315788
animals,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.46447368421052626,0.5460263157894737,0.34210526315789475,0.631578947368421,0.3986842105263158,0.5078947368421053
cars,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.8355,0.9144473684210527,0.631578947368421,0.9684210526315788,0.7473684210526316,0.9315789473684211
cars,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.6131578947368421,0.7029210526315789,0.3157894736842105,0.8157894736842105,0.45436842105263164,0.7302631578947368
monuments,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.7638947368421053,0.8257894736842105,0.2894736842105263,1.0,0.7048421052631578,0.9118421052631579
monuments,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.6032631578947368,0.6161842105263158,0.3684210526315789,0.7105263157894737,0.5135789473684211,0.5789473684210527
movies,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.7901363636363636,0.8242954545454545,0.5,0.9545454545454546,0.7113636363636364,0.8477272727272727
movies,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.5770000000000001,0.5898181818181819,0.3409090909090909,0.7272727272727273,0.5318181818181817,0.6681818181818182
