table_name,model,avg_valid_efficiency_score,avg_cell_precision,avg_cell_recall,avg_execution_accuracy,avg_tuple_cardinality,avg_tuple_constraint,avg_tuple_order
accounts_receivable,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.4442857142857143,0.432,0.0,0.7857142857142857,0.32110714285714287,0.562375
accounts_receivable,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.5272142857142857,0.5373571428571429,0.0,1.0,0.3841428571428572,0.717125
breast_cancer,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.8050357142857143,0.7720714285714285,0.2857142857142857,0.9642857142857143,0.3178571428571429,0.7008928571428571
breast_cancer,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.7676428571428572,0.7817142857142857,0.2857142857142857,1.0,0.31989285714285715,0.6964285714285714
fitness_trakers,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.5474285714285714,0.5832857142857143,0.17857142857142858,0.8571428571428571,0.3238214285714286,0.6411607142857143
fitness_trakers,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.5846428571428571,0.6387142857142857,0.07142857142857142,1.0,0.16667857142857143,0.625
heart_attack,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.8338571428571429,0.8555357142857142,0.5,1.0,0.5428571428571428,0.8338035714285714
heart_attack,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.7766785714285714,0.8089285714285713,0.32142857142857145,1.0,0.5476071428571428,0.8217142857142857
mobiles,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.3682142857142857,0.3424642857142857,0.21428571428571427,0.4642857142857143,0.23860714285714285,0.4107142857142857
mobiles,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.7009642857142857,0.7206071428571429,0.14285714285714285,0.8571428571428571,0.5469285714285714,0.539875
olympic_games,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.889,1.0,0.5,0.85825,0.95,1.0
olympic_games,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.90625,1.0,0.5,0.8,0.75,0.633875
