table_name,model,avg_valid_efficiency_score,avg_cell_precision,avg_cell_recall,avg_execution_accuracy,avg_tuple_cardinality,avg_tuple_constraint,avg_tuple_order
accounts_receivable,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.40267857142857144,0.40225,0.10714285714285714,0.7857142857142857,0.3392857142857143,0.6714285714285715
accounts_receivable,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.46260714285714283,0.4719285714285714,0.0,1.0,0.3375,0.7875
breast_cancer,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.6863928571428571,0.6901071428571429,0.39285714285714285,0.8928571428571429,0.5196428571428572,0.75
breast_cancer,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.4547142857142857,0.7105357142857143,0.10714285714285714,1.0,0.1142857142857143,0.5714285714285714
fitness_trakers,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.3767142857142857,0.47575,0.10714285714285714,0.8571428571428571,0.10714285714285714,0.5535714285714286
fitness_trakers,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.5223571428571429,0.5185357142857143,0.17857142857142858,1.0,0.19285714285714287,0.625
heart_attack,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.7273214285714286,0.7593571428571428,0.5357142857142857,0.9285714285714286,0.6285714285714287,0.8571428571428571
heart_attack,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.6762499999999999,0.7573571428571428,0.35714285714285715,1.0,0.55,0.8214285714285714
mobiles,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.64525,0.7035,0.35714285714285715,0.9642857142857143,0.5196428571428572,0.7678571428571429
mobiles,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.6446785714285713,0.6576428571428572,0.2857142857142857,1.0,0.5196428571428572,0.8089285714285713
olympic_games,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.95,0.96425,0.75,1.0,0.95,1.0
olympic_games,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.601,0.8875,0.25,1.0,0.45,0.6
