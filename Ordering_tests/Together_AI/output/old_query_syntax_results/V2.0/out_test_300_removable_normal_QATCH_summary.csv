table_name,model,avg_valid_efficiency_score,avg_cell_precision,avg_cell_recall,avg_execution_accuracy,avg_tuple_cardinality,avg_tuple_constraint,avg_tuple_order
accounts_receivable,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.4104642857142857,0.4090714285714286,0.03571428571428571,0.8571428571428571,0.33749999999999997,0.7910714285714285
accounts_receivable,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.43789285714285714,0.44282142857142853,0.0,1.0,0.33035714285714285,0.7017857142857142
breast_cancer,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.74625,0.7527499999999999,0.5,0.9642857142857143,0.6,0.7678571428571429
breast_cancer,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.5481071428571429,0.7058571428571428,0.14285714285714285,1.0,0.20357142857142857,0.625
fitness_trakers,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.48646428571428574,0.5298928571428572,0.17857142857142858,0.7714285714285715,0.37857142857142856,0.625
fitness_trakers,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.6701785714285714,0.6655,0.35714285714285715,1.0,0.4642857142857143,0.7678571428571429
heart_attack,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.7184642857142858,0.7161071428571428,0.5,0.8928571428571429,0.5642857142857143,0.8214285714285714
heart_attack,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.69125,0.7140714285714286,0.39285714285714285,1.0,0.5857142857142856,0.8357142857142856
mobiles,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.7065,0.7314285714285714,0.4642857142857143,1.0,0.5803571428571429,0.8392857142857143
mobiles,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.6789642857142857,0.6948928571428572,0.39285714285714285,1.0,0.6053571428571428,0.7999999999999999
olympic_games,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.91325,0.91325,0.5,1.0,0.9,1.0
olympic_games,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.96425,0.95,0.75,1.0,0.95,0.85
