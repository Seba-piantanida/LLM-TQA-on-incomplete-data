table_name,model,avg_valid_efficiency_score,avg_cell_precision,avg_cell_recall,avg_execution_accuracy,avg_tuple_cardinality,avg_tuple_constraint,avg_tuple_order
accounts_receivable,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.3986428571428572,0.4038214285714286,0.07142857142857142,0.7857142857142857,0.33035714285714285,0.6535714285714286
accounts_receivable,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.46014285714285713,0.46989285714285717,0.0,1.0,0.34464285714285714,0.8767857142857143
breast_cancer,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.6400714285714286,0.7371428571428572,0.25,0.9285714285714286,0.275,0.625
breast_cancer,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.4255,0.7203928571428572,0.07142857142857142,1.0,0.07857142857142858,0.5535714285714286
fitness_trakers,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.35514285714285715,0.4857857142857143,0.03571428571428571,0.7857142857142857,0.1357142857142857,0.5625
fitness_trakers,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.5307857142857143,0.5301785714285715,0.17857142857142858,1.0,0.19285714285714287,0.625
heart_attack,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.6920357142857142,0.7237142857142856,0.42857142857142855,0.8928571428571429,0.5785714285714285,0.7678571428571429
heart_attack,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.7199642857142857,0.8021428571428572,0.39285714285714285,1.0,0.6071428571428571,0.7964285714285715
mobiles,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.5675357142857143,0.6340714285714286,0.2857142857142857,0.8571428571428571,0.41428571428571426,0.6785714285714286
mobiles,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.6390357142857143,0.6537142857142857,0.2857142857142857,1.0,0.5053571428571428,0.7732142857142856
olympic_games,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.95,1.0,0.75,1.0,0.9375,1.0
olympic_games,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.601,0.8875,0.25,1.0,0.45,0.6
