table_name,model,avg_valid_efficiency_score,avg_cell_precision,avg_cell_recall,avg_execution_accuracy,avg_tuple_cardinality,avg_tuple_constraint,avg_tuple_order
animals,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.27742105263157896,0.3568157894736842,0.0,1.0,0.07368421052631578,0.631578947368421
animals,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.19460526315789475,0.26292105263157894,0.0,0.7105263157894737,0.06315789473684211,0.4473684210526316
cars,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.3598421052631579,0.47097368421052627,0.0,0.9736842105263158,0.14823684210526317,0.6710526315789473
cars,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.2147894736842105,0.2661842105263158,0.02631578947368421,0.7368421052631579,0.07105263157894737,0.4473684210526316
monuments,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.19326315789473686,0.2483421052631579,0.02631578947368421,1.0,0.09078947368421053,0.631578947368421
monuments,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.08297368421052631,0.11805263157894737,0.0,0.6842105263157895,0.005263157894736842,0.35526315789473684
movies,deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free,0.0,0.3790909090909091,0.4268863636363637,0.09090909090909091,0.9545454545454546,0.27045454545454545,0.7613636363636364
movies,meta-llama/Llama-3.3-70B-Instruct-Turbo-Free,0.0,0.23606818181818182,0.2625909090909091,0.0,0.6136363636363636,0.1340909090909091,0.4431818181818182
